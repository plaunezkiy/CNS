{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **The ring network**\n",
        "---\n",
        "\n",
        "*Learning outcomes*\n",
        "1. Be able to **numerically evaluate** the dynamical system of a **recurrent neural network**, including in the presence of **inputs** and **noise**\n",
        "2. Understand the relationship between the **connectivities** of the ring network and the **tuning** of its neurons\n",
        "3. Be able to describe why the **Hubel Wiesel**, **Uniform Inhibition** and **Marginal** regimes lead to qualitatively distinct behaviours of the network\n",
        "4. Understand how **noise** affects the different **regimes**"
      ],
      "metadata": {
        "id": "AS7BurNrTM2g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Grr4MkYL95"
      },
      "source": [
        "## Introduction\n",
        "The aim of this lab is to implement the ring network model which was originally proposed by [Ben-Yishai et al. (1995)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC42058/pdf/pnas01493-0220.pdf) to explain orientation tuning of neurons in visual cortex. As discussed in the lectures, this model exhibits distinct activity regimes in which orientation tuning\n",
        "is determined by:\n",
        "\n",
        "1. The tuning of feedforward inputs to the neurons.\n",
        "2. Recurrent synapses within the V1 circuit.\n",
        "\n",
        "Due to the behaviour of the ring model in the recurrent-dominated regime, it has since been adopted as a model for persistent activity of neurons observed in brain areas such as prefrontal cortex during the delay period of working memory tasks.\n",
        "\n",
        "<p align=\"center\" width=\"100%\">\n",
        "<img src=\"https://drive.google.com/uc?id=1Ntjz_giUoXliS-wg_va0Zkg56wE3Jg1c\"\n",
        "width=\"600px;\">\n",
        "</p>\n",
        "\n",
        "**Figure 1. The ring network.**  *Left:* The connectivities of the ring network. *Right:* (1) The high-dimensional network response occupies a low-dimensional continuum of states (a ring). (2) Equal changes in inputs produce equal changes in the encoded state. (3-4) The circle is an attractor, whereby states initialized away from the circle rapidly flow back. (5) States on the circle are energetically equal, with no net flow along the manifold. *Adapted from Fiete at al. [2019](https://www.nature.com/articles/s41593-019-0460-x), [2022](https://www.nature.com/articles/s41583-022-00642-0)*\n",
        "\n",
        "\n",
        "In the following exercises you will implement the model as a numerical simulation, vary the parameters in order to simulate the\n",
        "different regimes of the model, explore the behaviour of the model in each regime and think about possible implications of your findings for orientation tuning in visual cortex and for working memory in prefrontal cortex."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "### Continuous neuron\n",
        "The original model consists of a continuous ring of rate neurons which are parameterised by their preferred orientation $\\theta\\in (-\\pi/2,\\pi/2)$, as a featureless bar can only have distinguishable orientations in the range $-90^\\circ$ to $+90^\\circ$.\n",
        "\n",
        "The dynamics of the model is defined as,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\tau \\frac{dr (\\theta, t)}{dt}&=-r(\\theta, t) + [h(\\theta, t)]_{+}  \\hspace{1cm}(1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $[x]_{+}=x$ if $x>0$ and $0$ otherwise, that is the ReLU activation function. The net input is,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h(\\theta, t) &= u(\\theta-\\theta_s)+\\int_{-\\pi/2}^{\\pi/2} W(\\theta,\\theta')r(\\theta',t)d\\theta ' \\hspace{1cm}(2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\theta_s$ is the stimulus orientation. The recurrent weights are,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "W(\\theta,\\theta') &= A + B \\cos(2(\\theta-\\theta'))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and the feedforward external input,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "u(\\theta-\\theta_s) = C + D \\cos(2(\\theta-\\theta_s)) = c[1-\\epsilon+\\epsilon\\cos(2(\\theta-\\theta_s))]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where the second parameterisation of the input is interpreted as a stimulus contrast parameter $c$ and tuning strength parameter $\\epsilon$.\n",
        "\n",
        "### Discretisation\n",
        "This formulation of the model as a continuous network was chosen by Ben-Yishai et al. because it can be solved analytically to find the steady state responses (see the bonus section for details). However, for running simulations we need to convert the model into a network of N discrete neurons as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\tau \\frac{dr_i (t)}{dt}&=-r_i(t) + [h_i(t)]_{+}\n",
        "\\\\h_i(t) &= u_i+\\frac{1}{N}\\sum_{j=1}^N W_{ij}r_j(t)\n",
        "\\\\W_{ij}&=W_0 + W_1\\cos (2(\\theta_i-\\theta_j))\n",
        "\\\\u_i&=c[1-\\epsilon + \\epsilon \\cos(2(\\theta_i-\\theta_s))]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where neurons are indexed over a discrete set of angles $\\theta_i=i\\pi/N-\\pi/2$.\n"
      ],
      "metadata": {
        "id": "uCyAPlzAbU1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of the System\n",
        "This section is dedicated to implementing the dynamical system and how to evaluate it."
      ],
      "metadata": {
        "id": "36h89n7m-oM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZl1Ri63YL97"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "sns.set()\n",
        "\n",
        "# global defaults for plots - optional\n",
        "sns.set_theme(style=\"ticks\",\n",
        "              palette=\"colorblind\",\n",
        "              font_scale=1.7,\n",
        "              rc={\n",
        "              \"axes.spines.right\": False,\n",
        "              \"axes.spines.top\": False,\n",
        "          },\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1 $-$ Implementation of Functions\n",
        "\n",
        "Write code to implement the right hand side of each of the above equations."
      ],
      "metadata": {
        "id": "9AJ6AW3vQIzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the functions:\n",
        "\n",
        "def generate_u(N, Nt, c, epsilon, theta, theta_s):\n",
        "  '''\n",
        "  inputs\n",
        "  N: number of neurons\n",
        "  Nt: number of time steps\n",
        "  c, epsilon: input parameters\n",
        "  theta: N dimensional ndarray of preferred orientations\n",
        "  thea_s: stimulus orientation\n",
        "\n",
        "  output\n",
        "  u: NtxN dimensional ndarray [input to N neurons at Nt time points (in case varying stimulus over time)]\n",
        "  '''\n",
        "\n",
        "  # solution goes here\n",
        "\n",
        "  return u\n",
        "\n",
        "def h(r,u,W,t):\n",
        "  '''\n",
        "  inputs\n",
        "  r: N dimensional ndarray\n",
        "  u: NtxN dimensional ndarray\n",
        "  W: NxN dimensional ndarray\n",
        "  t: time point\n",
        "\n",
        "  output\n",
        "  h: N dimensional ndarray\n",
        "  '''\n",
        "\n",
        "def dr_dt(r,u,W,tau,t):\n",
        "  '''\n",
        "  inputs\n",
        "  r: N dimensional ndarray\n",
        "  u: NtxN dimensional ndarray\n",
        "  W: NxN dimensional ndarray\n",
        "  tau: time constant\n",
        "  t: time point\n",
        "\n",
        "  output\n",
        "  dr_dt: N dimensional ndarray\n",
        "  '''\n",
        "\n",
        "\n",
        "def generate_W(N, W0, W1, theta):\n",
        "  '''\n",
        "  inputs\n",
        "  N, W0, W1: model parameters (int)\n",
        "  theta: N dimensional ndarray\n",
        "\n",
        "  output\n",
        "  W: NxN dimensional ndarray\n",
        "  '''"
      ],
      "metadata": {
        "id": "_YC-32LLCLR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the system\n",
        "To simulate the dynamics in time, we will use the Euler method which we have implemented below. It works for any dynamical system that can be represented as $\\dot{\\bf{x}}=f(\\bf{x},t)$."
      ],
      "metadata": {
        "id": "JjMwZv2bJGbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euler(f,x0,Nt,dt):\n",
        "  \"\"\"\n",
        "  in:\n",
        "  f : RHS of the differential equation (dx(x)/dt), takes x as an input\n",
        "  x0 : initial state\n",
        "  dt : step size\n",
        "  Nt : number of steps\n",
        "\n",
        "  out:\n",
        "  xt : an Nt+1 * len(x0) matrix of the state of the system over time\n",
        "  \"\"\"\n",
        "\n",
        "  # solution goes here\n",
        "\n",
        "  return xt"
      ],
      "metadata": {
        "id": "xG7rmyEaC7-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper functions\n",
        "We define a function to simulate one run for the Ben-Yishai ring network with a time-dependent input, using the Euler method and starting from an initial condition $r_{init}$."
      ],
      "metadata": {
        "id": "2XJxSSE_OLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BY_ring_network(r_init, W, u, tau, Nt, dt):\n",
        "\n",
        "  # solution goes here\n"
      ],
      "metadata": {
        "id": "fjssJFKzOsmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation with Different Regimes\n",
        "\n",
        "To get a first impression for how the networks behave in their default settings, we next simulate and plot the activity of the network in three different regimes."
      ],
      "metadata": {
        "id": "UsC4cp63Q72G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default parameters\n",
        "As default parameters we set:\n",
        "\n",
        "\n",
        "*  $N=100$ the number discrete neurons\n",
        "*  $\\tau=10$ms the time constant of the network\n",
        "*  $\\theta_s = 0$ the stimulus orientation\n",
        "*  $c=0.5$ the external input strength (or stimulus contrast)\n",
        "*  $\\Delta t=1$ the time discretisation\n",
        "\n",
        "The values for $\\epsilon,W_0,W_1$ depend on the simulated regime and are defined in subsequent sections."
      ],
      "metadata": {
        "id": "4W_U8MDAT7m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 100\n",
        "tau_m = 10\n",
        "theta_s = 0\n",
        "c = 0.5\n",
        "dt = 1\n",
        "\n",
        "theta_p = ... # solution goes here"
      ],
      "metadata": {
        "id": "hR_962yNAyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bRvvYA4YL99"
      },
      "source": [
        "## Exercise 2.1 $-$ Hubel and Wiesel regime\n",
        "In the Hubel and Wiesel regime, the network is driven by strongly tuned feedforward inputs and there is no recurrent connectivity. To model this, set $W_0$ = 0, $W_1 = 0$ so that $h_i (t) = u_i$. As the input is strongly tuned, $\\epsilon$ is high. Set $\\epsilon$ = 1.\n",
        "\n",
        "1. What do you expect the steady state to look like in this regime? Write the steady state equation for $r_i$ from equation (1) and sketch the response you expect to the inputs $u_i$.\n",
        "\n",
        "Then, run the cell below and compare your prediction with the numerical results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6l9PBUrYL99"
      },
      "outputs": [],
      "source": [
        "Nt = 500\n",
        "epsilon = 1\n",
        "W_HW = generate_W(N, 0, 0, theta_p)\n",
        "u = generate_u(N, Nt, c, epsilon, theta_p, theta_s)\n",
        "\n",
        "output = BY_ring_network(np.zeros(N), W_HW, u, tau_m, Nt, dt)\n",
        "\n",
        "fig, ax = plt.subplots(1,2,  figsize = (20,5))\n",
        "sns.heatmap(output.T, ax = ax[0], xticklabels = 100, yticklabels = 20)\n",
        "ax[0].set_yticklabels(ax[0].get_yticklabels(), rotation=0)\n",
        "ax[0].set_title('Network output')\n",
        "ax[0].set_xlabel('Time (ms)')\n",
        "ax[0].set_ylabel('Neuron')\n",
        "\n",
        "ax[1].plot(np.degrees(theta_p), output[-1], label = '$r_i$')\n",
        "ax[1].plot(np.degrees(theta_p), u[-1], label = '$u_{i}$')\n",
        "ax[1].set_title('Steady state response')\n",
        "ax[1].set_ylabel('Network output')\n",
        "ax[1].set_xlabel('Preferred orientation ($^\\circ$)')\n",
        "ax[1].legend(bbox_to_anchor=(1., 1.0), loc='upper left', fontsize = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2.2 $-$ Uniform Inhibition regime\n",
        "\n",
        "The uniform inhibition regime was not discussed in lectures, but was included in the original paper of Ben-Yishai et al. This regime is defined by the uniform mutual inhibition between all neurons. This means that the recurrent weights are negative and do not differ around the ring. Therefore, set $W_0 = −1$ and $W_1 = 0$.\n",
        "\n",
        "1. As an exercise plot 1) the response at steady state and 2) the feedforward input $u_i$, for values of $\\epsilon$ as low as 0.01 and as high as 1.\n",
        "2.   How does $\\epsilon$ influence the results? Why do you think this happens?\n",
        "3.   For the experiments section below, pick $\\epsilon = 1$ when simulating the non-uniform regime (you can also explore the consequences of varying it if you like)."
      ],
      "metadata": {
        "id": "f6JuZDEkQ0bP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "t_bwsArfYL9-"
      },
      "outputs": [],
      "source": [
        "Nt = 500\n",
        "epsilons = np.linspace(0.01, 1, 4)   # try a range of different values for the input tuning parameter\n",
        "\n",
        "# Complete with the appropriate inputs to the function\n",
        "W_UI = generate_W(...)\n",
        "\n",
        "# Your solution here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2.3 $-$ Marginal Regime\n",
        "\n",
        "The above two regimes assume that the external input is strongly tuned and that recurrent input is either absent or uniform. Another theory is that the tuning of the inputs themselves is very weak, but is amplified by non-uniform interactions within the network. This regime is called the recurrent-dominated regime, or the “marginal” regime, which is a terminology inherited from models for spontaneous symmetry breaking in statistical physics.\n",
        "\n",
        "1. To model this case, set the external input tuning $\\epsilon$ to be small (e.g. 0.01), and set the connections to be strongly dependent on the preferred orientations of their pre- and post-synaptic\n",
        "neurons by choosing $W_0 = −1$, $W_1 = 3$. Simulate and plot as above.\n",
        "\n",
        "2. Try varying $\\epsilon$ to even smaller values (or larger ones), and investigate how this influences the profile of the network response and the input profile $u_i$."
      ],
      "metadata": {
        "id": "bRA5e9JyRG-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "f96d9UAFYL9_"
      },
      "outputs": [],
      "source": [
        "Nt = 500\n",
        "epsilons = np.linspace(0.01, 1, 4) #try varying this\n",
        "\n",
        "# Complete with the appropriate inputs to the function\n",
        "W_MR = generate_W(...)\n",
        "\n",
        "# Your solution here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare weight matrices\n",
        "\n",
        "To visualise how the weight matrices look like for the different regimes, we plotted them next to each other below*: the marginal regime is the only regime where the weight matrix shows any structure.  \n",
        "\n",
        "*Note that the code below will work only if the variables names `W_HW`, `W_UI`, `W_MR` were left unchanged in the previous cells"
      ],
      "metadata": {
        "id": "SlxYk50E27oD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhatFfB1YL9_"
      },
      "outputs": [],
      "source": [
        "Ws = [W_HW, W_UI, W_MR]\n",
        "regimes = ['Hubel and Wiesel', 'Uniform inhibition', 'Marginal Regime']\n",
        "\n",
        "fig, ax = plt.subplots(1,len(Ws), figsize = (20,5))\n",
        "\n",
        "for i in range(len(Ws)):\n",
        "  ax[i] = sns.heatmap(Ws[i], vmin = -4, vmax = 2, ax = ax[i], xticklabels = 20, yticklabels = 20)\n",
        "  ax[i].set_yticklabels(ax[i].get_yticklabels(), rotation=0)\n",
        "  ax[i].set_ylabel('Neuron')\n",
        "  ax[i].set_xlabel('Neuron')\n",
        "  ax[i].set_title(regimes[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "The following experiments are designed to reveal how the behaviour of the ring network differs between each of the above regimes. Run each experiment for the Hubel and Wiesel, uniform inhibition and marginal regime. For experiments involving changes in stimuli (rotation or deletion), make sure that the network has settled/stabilised prior to changing the stimulus."
      ],
      "metadata": {
        "id": "If6uxT5lRRuy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUgP_njyYL9_"
      },
      "source": [
        "## Exercise 3.1 $-$ Stimulus Rotation\n",
        "\n",
        "* Once the activity has settled under the presentation of a stimulus at $\\theta_s = 0$, rotate the stimulus by $60$ degrees.\n",
        "  1. How do you expect the activity to change after the rotation?\n",
        "  2. How do you think the change of activity depends on the network regime?\n",
        "  3. Do you think the behaviour would differ if you rotate the stimulus\n",
        "by different angles?\n",
        "\n",
        "* Examine the time course of the network activity in response to this stimulus rotation and compare the simulations with your predictions.\n",
        "\n",
        "There are two different possibilities on how to use the function BY_ring_network() to collect the results of a stimulus rotation. You can either define your stimulus for one whole simulation, so that the stimulus in itself already is changing (as shown below). Or you can run two seperate simulations with two different stimuli and use the last timepoint of the 1st simulation (using the 1st stimulus) as r_init for the second simulation (using the 2nd stimulus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "1yEpRIcZYL9_"
      },
      "outputs": [],
      "source": [
        "#Solution:\n",
        "\n",
        "Nt = 5000\n",
        "\n",
        "epsilon_HW = 1\n",
        "epsilon_UI = 1\n",
        "epsilon_MR = 0.01\n",
        "\n",
        "\n",
        "theta_s_1sthalf = 0\n",
        "theta_s_2ndhalf = np.pi/3 # 60 degrees rotation\n",
        "\n",
        "epsilons = [epsilon_HW, epsilon_UI, epsilon_MR]\n",
        "Ws = [W_HW, W_UI, W_MR]\n",
        "network_names = ['Hubel and Wiesel regime', 'Uniform Inhibition', 'Marginal Regime']\n",
        "\n",
        "# your solution here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3.2 $-$ Stimulus Deletion\n",
        "\n",
        "We next present a stimulus with orientation $\\theta_s = 0$. Once the activity has settled, the stimulus is deleted by setting $u_i = c$.\n",
        "\n",
        "1. How does the response following the deletion of the stimulus depend on the regime of the model?\n",
        "2. In which network regime could you decode the stimulus orientation after the stimulus has been deleted?"
      ],
      "metadata": {
        "id": "ofEWFsI_Ra29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "kKrtYcPlYL-A"
      },
      "outputs": [],
      "source": [
        "Nt = 5000\n",
        "\n",
        "epsilon_HW = 1\n",
        "epsilon_UI = 1\n",
        "epsilon_MR = 0.01\n",
        "\n",
        "\n",
        "theta_s = 0\n",
        "\n",
        "epsilons = [epsilon_HW, epsilon_UI, epsilon_MR]\n",
        "Ws = [W_HW, W_UI, W_MR]\n",
        "network_names = ['Hubel and Wiesel regime', 'Uniform Inhibition', 'Marginal Regime']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy Inputs\n",
        "\n",
        "As discussed in the lectures, neural activity is noisy. For the following experiments the total background noise is modelled as an additive Gaussian variable. Therefore we define $u_i (t)$ as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "u_i(t)&=c[1-\\epsilon+\\epsilon \\cos(2(\\theta-\\theta_s))]+\\sqrt{\\frac{\\tau}{\\Delta t}}\\sigma \\eta_i(t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\eta_i(t) \\sim \\mathcal{N}(0,1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "with $\\sigma$ the magnitude of the noise (we scale this by the discretisation timestep $\\Delta t$ and time constant $\\tau$ when numerically integrating to ensure invariance to the choice of units and to $\\Delta t$). To simulate, draw $\\eta_i$ independently for each neuron at each timestep."
      ],
      "metadata": {
        "id": "JwbmUQwsVRFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4.1 $-$ Persistent Noisy Input\n",
        "\n",
        "Simulate all three model regimes with the newly defined noisy input $u_i (t)$, and with different choices for $\\sigma$.\n",
        "1. How does the noise change the responses of the networks?\n",
        "2. Is there a network which is more robust to noise than the others?\n",
        "3. Try simulating the marginal regime but now with completely untuned inputs ($\\epsilon = 0$). How does noise influence the response of the network?\n",
        "4. Compare with noise values ranging from $\\sigma = 0.01$ to $σ = 1$. What do you think is happening? How does increasing $\\epsilon$ change the results?"
      ],
      "metadata": {
        "id": "jpYQUm9PReUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfewN2E-YL-A"
      },
      "outputs": [],
      "source": [
        "def generate_noisy_u(N, Nt, c, epsilon, theta, theta_s, sigma, rseed = 1, dt = 1, tau = 10):\n",
        "\n",
        "    u = np.zeros([Nt,N])\n",
        "    rng = np.random.RandomState(seed = rseed)\n",
        "\n",
        "    #Complete the code here:\n",
        "    u[:] = '...'\n",
        "\n",
        "    return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "JgENWAnTYL-A"
      },
      "outputs": [],
      "source": [
        "epsilon_HW = 1\n",
        "epsilon_UI = 1\n",
        "epsilon_MR = 0.01\n",
        "\n",
        "\n",
        "theta_s = 0\n",
        "\n",
        "epsilons = [epsilon_HW, epsilon_UI, epsilon_MR, 0]\n",
        "Ws = [W_HW, W_UI, W_MR, W_MR]\n",
        "\n",
        "network_names = ['Hubel and Wiesel regime', 'Uniform Inhibition', 'Marginal Regime', 'Marginal Regime - untuned input']\n",
        "\n",
        "sigmas = np.linspace(0.01, 1, 4)\n",
        "\n",
        "# Your solution here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4.2 $-$ Noisy Input After Stimulus Deletion\n",
        "\n",
        "Next we repeat the stimulus-deletion experiment, but this time include the persistent noisy input described above. Once the stimulus is deleted, the external input should be $u_i (t) = c+\\sqrt{\\frac{\\tau}{\\Delta t}} \\sigma\\eta_i (t)$.\n",
        "\n",
        "1. What happens in each regime?\n",
        "2. How would the noisy input influence the ability to decode the stimulus orientation after the stimulus has been deleted?\n",
        "3. How might this be relevant when modelling working memory (where stimuli need to be decoded after stimulus deletion)?"
      ],
      "metadata": {
        "id": "vcwqjdRbRlM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "EopqvZkOYL-B"
      },
      "outputs": [],
      "source": [
        "epsilon_HW = 1\n",
        "epsilon_UI = 1\n",
        "epsilon_MR = 0.01\n",
        "\n",
        "sigma = 0.1\n",
        "\n",
        "theta_s = 0\n",
        "network_names = ['Hubel and Wiesel regime', 'Uniform Inhibition', 'Marginal Regime']\n",
        "\n",
        "epsilons = [epsilon_HW, epsilon_UI, epsilon_MR]\n",
        "Ws = [W_HW, W_UI, W_MR]\n",
        "\n",
        "# your solution here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Section: Derivation of the Tuning Curves (Optional)\n",
        "\n",
        "In this section, we will derive the expression for the tuning curve of the neurons in the ring network.\n",
        "\n",
        "We consider the case where the stimulus is $\\theta_s=0$ to simplify notation. The steady state of equation (1) is $r^*(\\theta)$, which represents the activity profile of the network relative to the orientation of the stimulus. Thus, it also represents the tuning curve of a neuron, centred at its preferred orientation.\n",
        "\n",
        "The solution for the steady state is given by the equation $r^*(\\theta) = [h^*(\\theta)]_+$, where $h^*(\\theta)$ is given by equation (2) and is in the form:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h^*(\\theta) =& c[1-\\epsilon+\\epsilon\\cos(2(\\theta-\\theta_s))]\n",
        "+ W_0 \\int_{-\\pi/2}^{\\pi/2}  r^*(\\theta') d\\theta'\n",
        "+ W_1\\int_{-\\pi/2}^{\\pi/2}  \\cos(2(\\theta-\\theta'))r^*(\\theta')d\\theta' \\hspace{1cm}(S1)\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To find the solution for $r^*(\\theta)$, we need to move to the Fourier space.\n"
      ],
      "metadata": {
        "id": "3HjwoapsmYQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fourier series primer\n",
        "It is often possible to describe a function as an infinite sum of simpler functions. Such infinite sums are called *series*. You might be familiar with Taylor series, which allow writing a large class of functions as infinite sums of polynomials. Another useful series is [Fourier series](https://en.wikipedia.org/wiki/Fourier_series), which allows writing a large class of periodic functions as a sum of sinusoidal and cosinusoidal functions.\n",
        "\n",
        "For example, consider a P-periodic function $f(\\theta)$ for $\\theta\\in [-P/2,P/2]$. Assuming a Fourier series for $f$ exists, it can be written as,\n",
        "\n",
        "$$f(\\theta)=\\frac{a_0}{2} + \\sum_{k=1}^\\infty a_n\\cos \\left(\\frac{2\\pi k}{P}  \\theta\\right) + b_k\\sin \\left(\\frac{2\\pi k}{P}  \\theta\\right)$$\n",
        "\n",
        "where $P$ is the period of the function, that is $f(\\theta)=f(P\\theta)$ for all $\\theta$.\n",
        "\n",
        "Each term of this sum is called a *harmonic*. Thinking of the sinusoidal and cosinusoidal functions as elements of a vector space, they form an [orthonormal basis](https://en.wikipedia.org/wiki/Hilbert_space). Indeed, it is possible to define their inner product (a.k.a. dot product) and obtain the following orthogonality relations,\n",
        "\n",
        "$$\n",
        "\\int_{-P/2}^{P/2} \\cos \\left(\\frac{2\\pi k_1}{P} \\theta\\right) \\cdot \\cos \\left(\\frac{2\\pi k_2}{P} \\theta\\right) d\\theta = \\begin{cases}\n",
        "\\pi &\\text{if} &k_1 = k_2\n",
        "\\\\0 &\\text{if} &k_1\\neq k_2\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\int_{-P/2}^{P/2} \\sin \\left(\\frac{k_1}{P} \\theta\\right) \\cdot \\cos \\left(\\frac{k_2}{P} \\theta\\right) d\\theta &= 0 & \\forall k_1,k_2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "These relations allow us to derive the following equations for the *Fourier coefficients* $a_k,b_k$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a_k &= \\frac{2}{P}\\int_{-P/2}^{P/2}f(\\theta) \\cos (\\frac{2\\pi k}{P} \\theta)d\\theta &(i)\n",
        "\\\\b_k &= \\frac{2}{P}\\int_{-P/2}^{P/2}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta &(ii)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "A particularly relevant fact for the problem at hand is that even functions have $b_k=0 \\forall k$. (As a reminder an even functions satisfy $f(\\theta)=f(-\\theta)$ and an odd functions $-f(\\theta)=f(-\\theta)$). The proof is simple,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "b_k &= \\frac{2}{P}\\int_{-P/2}^{0}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta  + \\frac{2}{P}\\int_{0}^{P/2}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta  &\\text{By $(ii)$}\n",
        "\\\\&=\\frac{2}{P}\\int_{-P/2}^{0}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta  + \\frac{2}{P}\\int_{-P/2}^{0}f(-\\theta) \\sin (-\\frac{2\\pi k}{P} \\theta)d\\theta\n",
        "\\\\&=\\frac{2}{P}\\int_{-P/2}^{0}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta  - \\frac{2}{P}\\int_{-P/2}^{0}f(\\theta) \\sin (\\frac{2\\pi k}{P} \\theta)d\\theta &\\text{Since $f$ even and $\\sin$ odd}\n",
        "\\\\&=0.\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "dXH_EHSlH3vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Back to the ring\n",
        "\n",
        "The solution $r^*(\\theta)$ is periodic over the interval $\\theta\\in [-\\pi/2,\\pi/2]$. It is simpler to work with $\\phi=2\\theta\\in[-\\pi,\\pi]$. Since we have assumed that stimulus $\\theta_s=0$, $r^*(\\phi)$ must be an even function, and so we can write $r^*(\\phi)$ as a series of cosines:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "r^*(\\phi) &= \\sum_{k=0}^\\infty \\hat{r}_k \\cos\\left(k\\phi\\right),  \\hspace{1cm}(S2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\hat{r}_k$ are the Fourier coefficients of $r^*(\\phi)$.\n",
        "\n",
        "1. Substitute eq. (S2) into eq. (S1) to express it in terms of Fourier coefficients.\n",
        "\n",
        "\n",
        "2. Now substitute the previous equation for $h^*(\\theta)$ into $r^*(\\theta) = [h^*(\\theta)]_+$ and rearrange to show that the steady state response is $r^*(\\theta) = a[\\cos(2\\theta) - \\cos(2\\theta_c)]_+$ for some parameters $a,\\theta_c$ which depend on the Fourier coefficients $\\hat{r}_0, \\hat{r}_1$.\n"
      ],
      "metadata": {
        "id": "YaBqxqKsIfnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "[Ben-Yishai, R., Bar-Or, R. L., & Sompolinsky, H. (1995). Theory of orientation tuning in visual cortex. Proceedings of the National Academy of Sciences, 92(9), 3844-3848. ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC42058/)"
      ],
      "metadata": {
        "id": "VZ7HAxnbRwr2"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}