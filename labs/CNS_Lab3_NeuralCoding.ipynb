{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# **Neural coding**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "####*Learning outcomes*\n",
        "\n",
        "1. Understanding what a **tuning curve** is and how it relates sensory stimuli to neural activity.\n",
        "2. Knowing how to use a **Poisson process** to model variability in spiking.\n",
        "3. Understanding the relationship between the **mean, variance, bias and mean squared error** of an estimator.\n",
        "4. Knowing how to use the **Cramer-Rao** bound to bound the variance of an estimator."
      ],
      "metadata": {
        "id": "40YUff3Onp0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The main function of the brain is to make use of perceptual input to generate relevant behavioral output. In order to do this, it needs to create and manipulate informative representations of the world. Neural coding considers the problem of how information about the world is represented in the neural activity. More specifically, let $s$ be an external stimulus and $\\mathbf{r}=(r_1,r_2,...,r_M)$ the neural response it generates (e.g. firing rate of each neuron in a population of size $M$): *encoding* is the process by which the stimulus generates, in a stochastic manner, patterns of neural activity $p(\\mathbf{r}|s)$, while *decoding* is the reverse process, by which a neural population is “read out”, either by an experimenter or by downstream neurons, to produce an estimate of the stimulus $p(s|\\mathbf{r})$.\n",
        "\n",
        "For a single neuron, different values of $r$ will be recorded when $s$ is presented many times, and the mean response, denoted by $\\left<r\\right>=f(s)$, is called the *tuning curve* of the neuron. This function can typically be monotonic or bell‐shaped (e.g. for stimuli like orientation or position in space). When it is bell‐shaped, then the peak of the function is called the *preferred stimulus* of the neuron.\n",
        "\n",
        "Generally, different neurons have different tuning curves. Consider a set of neurons sensitive to a feature of the stimulus (e.g. neurons with different preferred orientations), then the information about the stimulus can be represented by the collective activity of this set of neurons. This is called a *population code*, and it can be useful for increasing the animal's certainty about a feature, as well as for encoding multiple features at once.\n",
        "\n",
        "In this lab we will focus on the decoding problem and compare different strategies of estimating, given a pattern of activity $\\mathbf{r}$, the stimulus $s$ that generated the response. We will therefore assume an encoding model $p(\\mathbf{r}|s)$ and ask how well we can estimate $\\hat{s}$ from the observed population response."
      ],
      "metadata": {
        "id": "pfAJpYKmny6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZl1Ri63YL97"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import scipy\n",
        "sns.set()\n",
        "\n",
        "# global defaults for plots - optional\n",
        "sns.set_theme(style=\"ticks\",\n",
        "              palette=\"Set2\",\n",
        "              font_scale=1.7,\n",
        "              rc={\n",
        "              \"axes.spines.right\": False,\n",
        "              \"axes.spines.top\": False,\n",
        "          },\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1.1 - Population of Poisson Neurons\n",
        "\n",
        "We first simulate one trial of the response of a population of (independent) Poisson neurons to stimulus *s*, which we will then try to decode. We consider the case of a circular stimulus $s\\in\\left[-180^\\circ, 180^\\circ\\right)$ (e.g. a visual bar oriented at a certain angle or the head direction of an animal). We model each neuron $i$ to have a preferred stimulus value $s^i_{pref}$ and tuning curve $f_i(s) = r_{max}e^{-(s-s^i_{pref})^2/(2\\sigma_f)^2}$, which describes the mean firing rate response to stimulus *s*. We assume the population has $\\{s^i_{pref}\\}_{i=1,...,M}$ uniformly distributed in the stimulus space.\n",
        "\n",
        "The total spike count $n_i$ for a stimulus presented over a time interval of length $T$ will have mean $\\left<n_i\\right>=\\left<r_i\\right>T=f_i(s)T$ and we assume the variability of $n_i$ around its mean in response to $s$ to be described as a Poisson process, so $ p(n_i|s) = \\frac {(f_i(s)T)^{n_i}e^{-f_i(s)T}}{n_i!} $.\n",
        "\n",
        " Set $r_{max} = 20 Hz$, $\\sigma_f=20$ and $T=1 s$.\n",
        "\n",
        "- Generate the response $\\mathbf{n}=(n_1,n_2,...,n_M)$ of a population of $M=92$ neurons (use `np.random.poisson`)\n",
        "- Plot the tuning curve of the neuron with $s_{pref}=0^\\circ$\n",
        "- Plot the population response to stimulus $s=45^\\circ$ (arrange the neurons on the x-axis based on their $s_{pref}$)"
      ],
      "metadata": {
        "id": "FSYMt3MINbrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_max = 20\n",
        "sigma_f = 20\n",
        "T = 1\n",
        "M = 92\n",
        "\n",
        "## your solution here\n",
        "# s = ...\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize = (10,5))\n",
        "\n",
        "# ax[0].plot(...)\n",
        "ax[0].set_xlabel('stimulus ($^\\circ$)'), ax[0].set_ylabel('f (spikes/s)')\n",
        "ax[0].set_title('Tuning curve for $s_{pref}=0^\\circ$')\n",
        "\n",
        "# ax[1].scatter(...)\n",
        "ax[1].set_xlabel('neurons ($s_{pref})$'), ax[1].set_ylabel('activity (spikes)')\n",
        "ax[1].legend(loc='upper left', fontsize = 10)\n",
        "ax[1].set_title(f'Response to $s={s}^\\circ$')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RRsXffnHwer_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoders\n",
        "\n",
        "Now we implement and compare three different decoding strategies discussed in the lectures.\n",
        "\n",
        "## Winner-take-all\n",
        "\n",
        "The simplest estimator of the stimulus from the population activity is the *winner-take-all* decoder, which takes as estimate $\\hat{s}$ the preferred stimulus value of the neuron with the highest response: $$\\hat{s} = s_j : j=\\underset{i}{\\mathrm{argmax}}\\ n_i$$\n",
        "\n",
        "* What are the limitations and possible failures of this strategy?\n",
        "\n",
        "## Population vector\n",
        "\n",
        "Another decoder is obtained by computing a weighted average of the preferred stimulus values of all neurons, with weights proportional to the responses of the respective neurons.\n",
        "\n",
        "For a circular stimulus space (and tuning curves), the appropriate average is the [circular mean](https://en.wikipedia.org/wiki/Circular_mean). In particular, the population vector decoder computes a weighted circular average by defining the *population vector* $\\mathbf{z} = \\sum_i n_i\\left[\\cos s^i_{pref}, \\sin s^i_{pref} \\right]$. This corresponds to representing each neural activity with a 2D vector whose angle matches the preferred stimulus of the neuron and whose magnitude is proportional to the strenght of the response. The vector sum of the population, which is equivalent to the weighted average over the angles, will generate a vector $\\mathbf{z}=[z1,z2]=[z\\cos\\hat{s}, z\\sin\\hat{s}]$ whose angle will represent the estimate of the decoded stimulus, so $$\\hat{s} = \\arctan\\left(\\frac{z2}{z1}\\right)$$\n",
        "\n",
        "* How do you think this compares to the winner-take-all in terms of performance and why? What are the limitations of this decoder?\n",
        "\n",
        "**Note**: The population vector decoder can only be defined properly on a set of *circular* tuning curves, which is not the case we are considering here. Examples of circular tuning curves would be, for example, rectified cosine functions $f_i(s) = \\frac{r_{max}}{1-r_0} [\\cos(s-s_{pref}^i) - r_0]_+$ or Von Mises curves $f_i(s) = r_{max} e^{\\cos(s-s_{pref}^i)/(2\\sigma^2)-1}$. However, for *narrow tuning curves* and stimulus values central in the interval (not close to the boundaries), the population decoder method can be applied to Gaussian tuning curves too (indeed, von Mises tuning curves become approximately Gaussian as $\\sigma\\rightarrow 0$).\n",
        "\n",
        "## Maximum Likelihood\n",
        "\n",
        "The previous two decoders are simple estimators that might perform well but they do not take into account the variability of neurons and how this might affect the estimate (e.g. the neuron with highest average firing rate for a given stimulus will not necessarily have the highest response on a single trial). A method that explicitly takes both the form of neuronal variability and tuning curves into account is the maximum‐likelihood decoder, which consists in computing the likelihood $p(\\mathbf{n}|s)$ of observing the response $\\mathbf{n}$ to stimulus $s$. The estimate of the stimulus is then the value that maximises this probability:\n",
        "$$ \\hat{s} = \\underset{s}{\\mathrm{argmax}}\\ p(\\mathbf{n}|s) $$\n",
        "\n",
        "Note that, since we assumed neural responses are independent, the likelihood of the population is given by $p(\\mathbf{n}|s) = \\prod_i p(n_i|s)$."
      ],
      "metadata": {
        "id": "tr5EKe2M_h99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1.2 - Estimate $\\hat{s}$\n",
        "\n",
        "* Given the response $\\mathbf{n}$ generated in the previous exercise, estimate $\\hat{s}$ with the three different decoders. How do they compare? Do they match your expectation? Try computing the ML estimate both numerically and analytically using the result derived in lectures ($\\hat{s}_{ML} = \\sum_i n_i s_{pref}^i/\\sum_i n_i$)."
      ],
      "metadata": {
        "id": "wst8VJthkM9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## estimate s_hat with the three decoding schemes\n",
        "\n",
        "## WTA\n",
        "# s_wta = ...\n",
        "\n",
        "## Population vector - convert from radians to degrees\n",
        "# ...\n",
        "# s_pop = ...\n",
        "\n",
        "## Maximum likelihood - compute over the range of stimuli\n",
        "# ...\n",
        "# s_ml = ...\n",
        "# s_ml_analytical = ...\n",
        "\n",
        "print(\"Stimulus estimates (in degrees)\")\n",
        "print(f\"WTA decoder: {s_wta:.2f}\")\n",
        "print(f\"Population vector decoder: {s_pop:.2f}\")\n",
        "print(f\"ML decoder numerical: {s_ml:.2f}\")\n",
        "print(f\"ML decoder analytical: {s_ml_analytical:.2f}\")\n"
      ],
      "metadata": {
        "id": "cfEQkE0rUKek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2.1 - Bias and variance of decoders\n",
        "\n",
        "We can now compare the bias and variance of these three decoders. In order to do that, we simulate $N_{trials} = 10000$ for a fixed value of $s$ and compute (numerically) the bias $b(s) = \\langle \\hat{s} - s \\rangle$, variance $var(s) = \\langle (\\hat{s} - \\langle \\hat{s}\\rangle )^2 \\rangle$, and mean squared error  $MSE(s) = \\langle (\\hat{s} -  s )^2 \\rangle$ for each decoder (averages are with respect to $p(n|s)$ for fixed $s$). Compute these quantities for the three decoders for $s=45^\\circ$.\n",
        "\n",
        "* How do they compare? Does this match your expectation?"
      ],
      "metadata": {
        "id": "f_V9g3Ce47Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ntrials = 10000\n",
        "r_max = 20\n",
        "sigma_f = 20\n",
        "T = 1\n",
        "M = 92\n",
        "\n",
        "s = 45\n",
        "\n",
        "s_dec_wta = np.zeros([Ntrials,1])\n",
        "s_dec_pop = np.zeros([Ntrials,1])\n",
        "s_dec_ml = np.zeros([Ntrials,1])\n",
        "s_dec_ml_analytical = np.zeros([Ntrials,1])\n",
        "\n",
        "# ...\n",
        "\n",
        "for t in range(Ntrials):\n",
        "    # ...\n",
        "    # s_dec_wta[t] = ...\n",
        "    # s_dec_pop[t] = ...\n",
        "    # s_dec_ml[t] = ...\n",
        "    # s_dec_ml_analytical[t] = ...\n",
        "\n",
        "# b_wta = ...\n",
        "# b_pop = ...\n",
        "# b_ml = ...\n",
        "# b_ml_analytical = ...\n",
        "# var_wta = ...\n",
        "# var_pop = ...\n",
        "# var_ml = ...\n",
        "# var_ml_analytical = ...\n",
        "# mse_wta = ...\n",
        "# mse_pop = ...\n",
        "# mse_ml = ...\n",
        "# mse_ml_analytical = ...\n",
        "\n",
        "print(f\"Bias: \\t {b_wta:.3f} (WTA), \\t {b_pop:.3f} (PopVector), \\t {b_ml:.3f} (ML), \\t {b_ml_analytical:.3f} (ML analytical)\")\n",
        "print(f\"Var: \\t {var_wta:.3f} (WTA), \\t {var_pop:.3f} (PopVector), \\t {var_ml:.3f} (ML), \\t {var_ml_analytical:.3f} (ML analytical)\")\n",
        "print(f\"MSE: \\t {mse_wta:.3f} (WTA), \\t {mse_pop:.3f} (PopVector), \\t {mse_ml:.3f} (ML), \\t {mse_ml_analytical:.3f} (ML analytical)\")\n"
      ],
      "metadata": {
        "id": "-Q-Qjgzk7Sv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2.2 - Cramer-Rao bound\n",
        "\n",
        "## Variance $-$ MSE $-$ Fisher Information\n",
        "Suppose we are working with an **unbiased estimator**. Namely, $b(s)\n",
        "= \\langle \\hat s \\rangle - s = 0$, that is the expected value of the estimator is the true stimulus. This directly implies that the **variance** of the etimator:\n",
        "$$\n",
        "\\text{var} (s) = \\langle (\\hat s - \\langle \\hat s\\rangle)^2 \\rangle = \\langle (\\hat s - s)^2 \\rangle = MSE(s)\n",
        "$$\n",
        "that is the **mean squared error** of the estimator. The Cramer-Rao bound states that this value is lower-bounded by the Fisher information of the stimulus.\n",
        "$$\n",
        "\\text{var}(s) \\geq \\frac{1}{I_F(s)}\n",
        "$$\n",
        "This becomes an equality if the estimator is [optimal](https://en.wikipedia.org/wiki/Efficiency_(statistics)). Namely, *the Fisher information is the reciprocal of the mean squared error (or variance) of an optimal estimator*.\n",
        "\n",
        "## Fisher information of the tuning curve + Poisson noise model\n",
        "As a first exercise, you will compare the Cramer-Rao lower bound to the variance of your three above estimators. For this, you will need to compute the Fisher information of the stimulus. As a reminder if $p(n|s):=\\text{Poisson}(n | f(s)T) = (f(s)T)^n e^{-f(s)T}/n!$ then:\n",
        "$$\n",
        "I_F(s) :=  - \\left\\langle \\frac{d^2}{ds^2} \\log p(n | s) \\right\\rangle =  \\frac{f'(s)^2}{f(s)}T\n",
        "$$\n",
        "\n",
        "Here, for the sake of analytical simplicity we consider *Gaussian tuning curves*. That is $f_i(s) = r_{max}e^{-(s-s_i)^2/(2\\sigma)^2}$.\n",
        "\n",
        "* Derive the Fisher information $I_F(s,i)$ of a single neuron tuned according to $f$.\n",
        "\n",
        "* Additionally, for an unbiased estimator, $var(s)\\geq \\sum_i^M I_F(s,i):=I_F(s)$. Derive the expression for $I_F(s)$, the bound on the variance of an unbiased estimator from the population activity.\n",
        "\n",
        "* Numerically compare the Cramer-Rao bound with the estimators' variance. What is missing from this analysis?\n",
        "\n",
        "* Vary the number of neurons and the width of the tuning curves (separately) and plot how the Cramer-Rao bound changes."
      ],
      "metadata": {
        "id": "1w0CNiT95eZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution\n",
        "\n",
        "r_max = 20\n",
        "sigma_f = 20\n",
        "T = 1\n",
        "M = 92\n",
        "\n",
        "s = 45\n",
        "s_pref = np.linspace(-180, 180, M)\n",
        "s_range = np.linspace(-180, 180, 500)\n",
        "rng = np.random.RandomState(seed=42)\n",
        "\n",
        "# def cramer_rao(s): ...\n",
        "\n",
        "Ms = np.arange(10, 101, 1)\n",
        "bounds_M = []\n",
        "\n",
        "for M_temp in Ms:\n",
        "  s_pref_temp = np.linspace(-180, 180, M_temp)\n",
        "  # def I_F(s): ...\n",
        "  bounds_M.append(cramer_rao(s))\n",
        "\n",
        "sigma_fs = np.arange(10, 21, 1)\n",
        "s_pref = np.linspace(-180, 180, M)\n",
        "bounds_sigma = []\n",
        "for sigma_f_temp in sigma_fs:\n",
        "  # def I_F(s): ...\n",
        "  bounds_sigma.append(cramer_rao(s))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize = (10,5))\n",
        "ax[0].plot(Ms, bounds_M)\n",
        "ax[0].set_xlabel('M')\n",
        "ax[0].set_ylabel('Cramer-Rao')\n",
        "ax[0].set_title('$\\sigma=20$')\n",
        "\n",
        "ax[1].plot(sigma_fs, bounds_sigma)\n",
        "ax[1].set_xlabel('$\\sigma$')\n",
        "ax[1].set_ylabel('Cramer-Rao')\n",
        "ax[1].set_title('$M=92$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8041rhjfIbkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Biased estimators\n",
        "\n",
        "The Cramer-Rao bound can be generalized to biased estimators:\n",
        "$$\n",
        "\\text{var}(s) \\geq \\left (1+b'(s)\\right )^2I_F(s)^{-1}\n",
        "$$\n",
        "where $\\langle \\hat s - s\\rangle = b(s)$ is the bias.\n",
        "\n",
        "---\n",
        "**Optional** *: proof of Cramer-Rao for biased estimators.* Suppose $\\langle \\hat s - s\\rangle = b(s)$ then $\\langle \\hat s - (s-b(s))\\rangle = 0$ and therefore $\\hat s$ is an unbiased estimate of $\\bar s = s+b(s)$. We can therefore apply our previous bound to find:\n",
        "$$\n",
        "\\text{var}(s) \\geq \\frac{1}{I_F(\\bar s)}\n",
        "$$\n",
        "We would however like to obtain an expression bounding the variance in terms of the Fisher information of $s$ rather than $\\bar s$. For this, we return to the definition of the Fisher information:\n",
        "$$\n",
        "\\begin{align*}\n",
        "I_F(\\bar s) =& -E\\left[\\frac{\\partial^2 }{\\partial \\bar s^2}\\log p(x|s) \\right] \\\\\n",
        "=& -E\\left[\\frac{\\partial}{\\partial \\bar s}\\left (\\frac{\\partial \\log p(x|s)}{\\partial s}\\frac{\\partial s}{\\partial \\bar s}\\right ) \\right] \\\\\n",
        "=& -E\\left[\\frac{\\partial^2 \\log p(x|s)}{\\partial s^2}\\left(\\frac{\\partial s}{\\partial \\bar s}\\right)^2 + \\frac{\\partial \\log p(x|s)}{\\partial s}\\frac{\\partial^2 s}{\\partial \\bar s^2} \\right] \\\\\n",
        "= & -\\left(\\frac{\\partial s}{\\partial \\bar s}\\right)^2 E\\left[\\frac{\\partial^2 \\log p(x|s)}{\\partial s^2}\\right] - \\frac{\\partial^2 s}{\\partial \\bar s^2}E\\left[\\frac{\\frac{\\partial}{\\partial s}p(x|s)}{p(x|s)} \\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "Now notice that $E\\left[\\frac{\\frac{\\partial}{\\partial s}p(x|s)}{p(x|s)} \\right]=\\int_\\Omega \\frac{\\frac{\\partial}{\\partial s}p(x|s)}{p(x|s)} p(x|s)dx=\\frac{\\partial}{\\partial s}\\int_\\Omega p(x|s)dx=\\frac{\\partial}{\\partial s} 1 = 0$. Furthermore $\\frac{ds}{d\\bar s}=(1+b'(s))^{-1}$ and $E\\left[\\frac{\\partial^2 \\log p(x|s)}{\\partial s^2}\\right]$ is simply the Fisher information of the stimulus $I_F(s)$. Thus:\n",
        "$$\n",
        "I_F(\\bar s) = (1+b'(s))^{-2} I_F(s)\n",
        "$$\n",
        "and finally, using the expression we started with:\n",
        "$$\n",
        "\\text{var}(s) \\geq \\left (1+b'(s)\\right )^2I_F(s)^{-1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "* Derive an expression for the variance in terms of the MSE and bias.\n",
        "\n",
        "* Use the above expression to bound the MSE.\n",
        "\n",
        "* In lectures we computed the Fisher information for the limit of continuously many neurons with density $\\rho = M/360^\\circ$ and found that $I_F(s) \\rightarrow \\frac{\\sqrt{2\\pi} \\rho r_{max} T}{\\sigma}$. Compare this quantity numerically to the variance obtained above for a finite number of neurons $M$. Try varying $M$ to check the validity of the approximation."
      ],
      "metadata": {
        "id": "AIMWOwhnGi-V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vxJBF_h3iFMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}